import torch
from ocr_medical.core.status import status_manager

def load_waifu2x(
    # ---- Tham s·ªë quan tr·ªçng ----
    model_type="art_scan",  # ki·ªÉu model:
                            #   'art'       = tranh, anime, ch·ªØ (OCR th∆∞·ªùng ch·ªçn c√°i n√†y)
                            #   'photo'     = ·∫£nh ch·ª•p
                            #   'art_scan'  = scan gi·∫•y/t√†i li·ªáu

    method="noise_scale",   # ch·∫ø ƒë·ªô x·ª≠ l√Ω:
                            #   'scale'        = ch·ªâ ph√≥ng to
                            #   'noise'        = ch·ªâ kh·ª≠ nhi·ªÖu (kh√¥ng ƒë·ªïi size)
                            #   'noise_sca  le'  = kh·ª≠ nhi·ªÖu + ph√≥ng to (th∆∞·ªùng d√πng cho OCR)
                            #   'auto_scale'   = t·ª± ƒë·ªông ch·ªçn scale theo input
    
    noise_level=3,  # m·ª©c kh·ª≠ nhi·ªÖu:
                    #   -1 = t·∫Øt
                    #    0 = none/very low
                    #    1 = low
                    #    2 = medium
                    #    3 = high

    scale=2,    # h·ªá s·ªë ph√≥ng to:
                #   1 (no upscale), 1.6, 2, 4 (tu·ª≥ model h·ªó tr·ª£)

    # ---- T·ªëi ∆∞u hi·ªáu nƒÉng ----
    tile_size=64,       # chia ·∫£nh th√†nh tile 64/128/256/400/640 (VRAM th·∫•p th√¨ gi·∫£m)
    batch_size=4,       # s·ªë tile x·ª≠ l√Ω song song (tƒÉng khi c√≥ nhi·ªÅu VRAM)
    device_ids=[-1],    # [-1] = CPU, [0] = GPU, [0,1] = multi-GPU          
    amp=True,           # True = d√πng FP16 (tƒÉng t·ªëc, gi·∫£m VRAM); ch·ªâ ho·∫°t ƒë·ªông tr√™n GPU NVIDIA    


    # ---- ƒê∆∞·ªùng d·∫´n ----
    source="github",        # cho ph√©p ƒë·ªïi sang 'local' n·∫øu mu·ªën
    repo="nagadomi/nunif",  # ƒë∆∞·ªùng d·∫´n repo local ho·∫∑c 'nagadomi/nunif' tr√™n GitHub
):
    try:
        status_manager.add("üîÑ ƒêang load model Waifu2x...")
        upscaler = torch.hub.load(
            repo,
            'waifu2x',
            source=source,
            model_type=model_type,
            method=method,
            noise_level=noise_level,
            scale=scale,
            tile_size=tile_size,
            batch_size=batch_size,
            device_ids=device_ids,
            amp=amp
        )
        status_manager.add("‚úÖ Load model Waifu2x th√†nh c√¥ng")
        return upscaler
    except Exception as e:
        status_manager.add(f"‚ùå L·ªói load model Waifu2x: {e}")
        raise
